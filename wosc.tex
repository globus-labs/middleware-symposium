\documentclass[sigconf, 9pt]{acmart}


\usepackage{xcolor}

\usepackage{float}
\usepackage{siunitx}


\newif\iffinal

% \finaltrue

\iffinal
  \newcommand{\tyler}[1]{}
  \newcommand{\ian}[1]{}
  \newcommand{\kyle}[1]{}
\else
  \newcommand{\tyler}[1]{{\textcolor{cyan}{ tyler: #1 }}}
  \newcommand{\ian}[1]{{\textcolor{red}{ Ian: #1 }}}
  \newcommand{\kyle}[1]{{\textcolor{purple}{ Kyle: #1 }}}
\fi


%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}


\newcommand{\name}{Xtract}
\newcommand{\funcx}{$f$\kern-0.18em\emph{unc}\kern-0.05em X}

\acmConference{Middleware Doctoral Symposium}{2019}{Davis, CA}

\begin{document}


\title{Leveraging Serverless to Dynamically Extract Metadata from Distributed Data at the Edge}

\author{Tyler J. Skluzacek} 
\affiliation{University of Chicago}
\email{skluzacek@uchicago.edu}



\renewcommand{\shortauthors}{Skluzacek et al.}

\begin{abstract}

The rapid generation of data from distributed IoT devices, scientific instruments, and compute clusters presents
unique challenges in data management. The influx of massive, increasingly-complex data causes repositories to become 
siloed or generally unsearchable---both problems not currently well-addressed by distributed file systems.  
In this work we propose \name{}, a serverless middleware 
to extract inscribed metadata from files across heterogeneous edge computing resources. Moreover, we intend to study how \name{} can
automatically construct file extraction workflows subject to users' cost, time, compute allocation preference, and security constraints. 
To this end, \name{} creates a searchable centralized index across distributed data collections.


\end{abstract}

\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10002951.10003227.10010926</concept_id>
<concept_desc>Information systems~Computing platforms</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10002951.10003317.10003365.10003366</concept_id>
<concept_desc>Information systems~Search engine indexing</concept_desc>
<concept_significance>300</concept_significance>
</concept>
<concept>
<concept_id>10010405.10010497.10010500.10010503</concept_id>
<concept_desc>Applied computing~Document metadata</concept_desc>
<concept_significance>500</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Information systems~Computing platforms}
\ccsdesc[500]{Applied computing~Document metadata}
\ccsdesc[300]{Information systems~Search engine indexing}

\keywords{data lakes, serverless, metadata extraction, file systems}

\maketitle


\section{Introduction}

The rapid generation of data from IoT devices, scientific instruments, and other computers presents unique 
data management challenges. Currently data exist across multiple machines, are generally siloed, and require 
significant manual labor by users to create metadata that promote usability and searchability. Some~\cite{egan2003vizier, dataverse}  have created data catalogs from user-submitted metadata. These, however, are not scalable to current and future storage systems,
as humans cannot possibly label billions of heterogeneous files.   
In order to build a global metadata catalog over distributed big data, we require automated methods to crawl file systems and extract 
metadata attributes for each file therein. Others have developed end-to-end 
automated metadata extraction systems, but require that data be moved to a central service~\cite{skluzacek2018skluma, skluzacek2016klimatic, padhy2015brown, rodrigo2018sciencesearch} or do not have built-in scaling capabilities~\cite{mattmann2011tika}. 
In this work we strive to create a scalable, decentralized metadata extraction system that eschews transferring files to a central service.  

We present our prototype and vision for \name{},
a serverless middleware that provides high-throughput and on-demand metadata 
extraction, enabling the automated creation of rich, searchable data lakes from unsearchable data siloes. 
Serverless computing, and specifically function as a service (FaaS),
provides an ideal model for managing the invocation of
many short-running extractors on an arbitrarily large number of files. 
\name{} uses the \funcx{} serverless supercomputing platform~\cite{chard2019serverless}
to execute functions across diverse and distributed computing infrastructure.
Rather than rely on commercial FaaS systems, we explore a novel distributed FaaS model 
that overcomes the limitation of moving large amounts of data to the cloud. 
Instead, we are able to push
metadata extractors to the edge systems on which the scientific data reside. 
The primary contributions of \name{} are the following: 
\begin{itemize}
\item Scalable across all user resources, from IoT to HPC. 
\item Deployable at the edge, facilitating decentralized metadata extraction.
\item Dynamically constructs unique extractor pipelines on diverse file types. 
\item Intelligently executes data staging decisions based on user-supplied constraints, including time, cost, compute allocation availability, and security. 
\item Automatically populates a search index of rich, searchable metadata. 
\end{itemize}



\section{Approach}
\label{sec:approach}

\name{} is a serverless middleware that runs atop the \funcx{} serverless 
computing platform, allowing users to automatically optimize metadata extraction workflows subject to 
a number of user constraints, including extraction time, cost, available compute allocations, and security policies. 
An example two-site architecture of \name{} is shown in Figure~\ref{fig:arch}, and the remainder of this section 
details its various components and design goals.

\textbf{Metadata extractors} are functions that input a file or group of files, and output a metadata dictionary. 
Each metadata extractor runs in a given container runtime with all required dependencies (i.e., files and 
libraries).  \name{} currently provides a number of built-in extractors, including
those to identify null values in tabular files, nesting patterns in structured XML files, topics and keywords from free text, and location tags from map images, 
among others. In future work, 
we plan to support user-submitted metadata extractors, automatically generate (and potentially share) runtime containers based on inferred 
dependency requirements, and train \name{} to recognize when it is appropriate to apply user-submitted extractors in each file's metadata extraction workflow. 

The \textbf{\name{} service} prototype dynamically applies a set of metadata extractors to each file in the repository. 
First, \name{} sends a crawler function to the data and populates a metadata dictionary for each file containing
its physical properties (e.g., path, size, extension, hash).  Once the initial dictionary is created, \name{} invokes a file type extractor on each 
file that determines probable downstream extractors for each file. We have shown that feeding 
the first `n' bytes of a file as features into 
a trained model can predict the appropriate first extractor functions to apply to a file, in significantly less time
than attempting to execute incorrect extractors on each file~\cite{skluzacek2018skluma}. Once this first extractor function returns metadata, the \name{} service dynamically selects additional extractors to apply based on this output.  For instance, 
a tabular file with a multi-line free text header (e.g., describing experimental setup) is identified by the system as first requiring a tabular extractor, but 
the resultant metadata will denote free text parts of the file that can benefit from keyword and topic extractors.
\name{} protects requests to the web service using Globus Auth~\cite{tuecke2016globus}, and stores metadata to a Globus Search index. 

\funcx{} \textbf{endpoints} are the edge computing fabric that provision compute resources and execute functions in container runtimes for files on the local file system.
 Endpoints can currently be deployed across myriad compute providers such as IoT devices, cloud providers, and HPC centers.  
Endpoints utilize the Parsl~\cite{babuji2019parsl} parallel programming library to 
provision compute resources, and to manage the execution of functions in containers on said resources. \funcx{} enables 
\name{} to execute metadata extraction functions at any registered and accessible \funcx{} endpoint, and reliably
scale to deploy millions of metadata extraction functions to thousands of nodes spanning multiple compute locations. 
%\funcx{} supports Docker, Singularity, and Shifter, allowing \name{} extractors to be executed
%on various high performance computing systems. 

\begin{figure}[t]
	\centering
	\includegraphics[scale=0.17]{figs/updated-fig.png}
	\caption{Overview of the \name{} architecture. For \textit{Site A} functions are transmitted to the remote resource and performed on local computing resources, returning metadata to the \name{} service. \textit{Site B} lacks suitable local
	computing capabilities, requiring data to be staged to \name{} for analysis.}
	\label{fig:arch}
\end{figure}


\section{Evaluation Plan}
\label{sec:eval}

In future work we plan to study how \name{} can self-optimize subject to real-world, user-defined constraints for heterogeneous data stored across multiple 
endpoints.  Specifically, we plan to explore how augmenting metadata extraction workflows to stage data to idle or under-utilized resources could 
find globally optimal extraction strategies with respect to diverse user constraints of financial cost, computing time, security, and compute resource allocation 
availability or volatility.

We intend to evaluate \name{}'s self-optimization strategies subject to a large number of constraint combinations across a diversity of datasets. 
These include the Carbon Dioxide Information Analysis Center (330+ GB, 10,000+ 
unique file extensions of carbon dioxide data), the Materials Data Facility~\cite{ blaiszik2019mdf} (30+ TB, tens of millions of materials science files); 
Petrel (4+ PB, 50,000+ files of cross-disciplinary data at Argonne National Lab), and Globus (20,000+ unique, connected 
endpoints containing hundreds of billions of files).


\section{Conclusion}
\label{sec:conc}

\name{} enables users to support the search and organization of large, distributed data sets by dynamically constructing metadata extraction 
workflows for each file. \name{} is a serverless middleware that solves the data locality and scalability challenges by deploying compute endpoints on the data's native edge
devices and constructing extraction workflows subject to a number of user constraints. Such a system will enable companies, researchers, and individuals alike to more easily find, understand, and share increasingly 
complex data sets, leading to enhanced scientific and industrial progress. 


\begin{acks}

This research is conducted under the guidance of Dr. Ian Foster and Dr. Kyle Chard, and with contributions
from Dr. Ryan Chard, Dr. Zhuozhao Li, Yadu Babuji, and Ryan Wong. 


\end{acks}
\bibliographystyle{ACM-Reference-Format}
\bibliography{wosc}


\end{document}
\endinput
