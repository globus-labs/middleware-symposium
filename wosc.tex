\documentclass[sigconf]{acmart}


\usepackage{xcolor}

\usepackage{float}
\usepackage{siunitx}


\newif\iffinal

% \finaltrue

\iffinal
  \newcommand{\tyler}[1]{}
  \newcommand{\ian}[1]{}
  \newcommand{\kyle}[1]{}
\else
  \newcommand{\tyler}[1]{{\textcolor{cyan}{ tyler: #1 }}}
  \newcommand{\ian}[1]{{\textcolor{red}{ Ian: #1 }}}
  \newcommand{\kyle}[1]{{\textcolor{purple}{ Kyle: #1 }}}
\fi


%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}


\newcommand{\name}{Xtract}
\newcommand{\funcx}{$f$\kern-0.18em\emph{unc}\kern-0.05em X}

\acmConference{Fifth International Workshop on Serverless Computing (WoSC)}{2019}{Davis, CA}

\begin{document}


\title{Harnessing Serverless to Extract File Metadata at the Edge}


\author{Tyler J. Skluzacek} 
\affiliation{University of Chicago}
\email{skluzacek@uchicago.edu}



\renewcommand{\shortauthors}{Skluzacek et al.}

\begin{abstract}
\tyler{max 100 words}

\tyler{GRAND-er. be more dramatic. }
The rapid increase in the number of data sources and sizes will prove largely 
useless if these data are not findable, interpretable, or reusable. In this work 
we propose a middleware architecture called Xtract that leverages serverless 
computing to simultaneously extract metadata across a continuum of heterogeneous edge 
devices, from IoT to HPC to 
laptops. Xtract will enable the curation of an atomic multi-site data lake in which the
resources can make intelligently scale and move data in order to optimize for various 
SLAs.


\end{abstract}

\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10002951.10003227.10010926</concept_id>
<concept_desc>Information systems~Computing platforms</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10002951.10003317.10003365.10003366</concept_id>
<concept_desc>Information systems~Search engine indexing</concept_desc>
<concept_significance>300</concept_significance>
</concept>
<concept>
<concept_id>10002951.10003317.10003318.10003319</concept_id>
<concept_desc>Information systems~Document structure</concept_desc>
<concept_significance>100</concept_significance>
</concept>
<concept>
<concept_id>10010405.10010497.10010500.10010503</concept_id>
<concept_desc>Applied computing~Document metadata</concept_desc>
<concept_significance>500</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Information systems~Computing platforms}
\ccsdesc[500]{Applied computing~Document metadata}
\ccsdesc[300]{Information systems~Search engine indexing}
\ccsdesc[100]{Information systems~Document structure}

\keywords{data lakes, serverless, metadata extraction, file systems, materials science}

\maketitle



\section{Contributions}

\tyler{merge these (as bullets?) into intro text}
The primary goal of this work is to both implement and evaluate a serverless metadata extraction system
that allows users to deploy endpoints at the edge on which metadata extraction functions can be run, 
creating a comprehensive, searchable index of files across multiple edge devices. Moreove, we plan to 
explore how the system can intelligently self-optimize subject to a number of constraints, including compute time, 
current allocation availability, monetary cost, and security requirements (e.g., HIPAA). 

\section{Introduction}

\tyler{Intro lead-in here}


\name{} builds upon a large body of work in both metadata extraction systems and serverless computing.
Others~\cite{egan2003vizier, welter2013nhgri, irods, dataverse} have created 
systems to manage metadata catalogs that
support the organization and discovery of research data. However, these approaches typically 
require that users provide metadata and that curators continue to organize data over time. 
A number of systems exist for automatically extracting metadata from repositories. 
For example, ScienceSearch~\cite{rodrigo2018sciencesearch} uses 
machine learning techniques to extract metadata from a dataset served by the National Center for Electron Microscopy (NCEM). 
Most of data in this use case are micrograph images, but additional contextual metadata are derived from file system 
data and free text proposals and publications. Like \name{}, ScienceSearch 
provides a means for users to extensibly switch metadata extractors to suit a given dataset.  
Brown Dog~\cite{padhy2015brown} is an extensible metadata extraction platform, 
providing metadata extraction services for a number of 
disciplines ranging from materials science to social science.
Unlike \name{}, Brown Dog requires that files are uploaded for extraction. 

While most related research performs metadata extraction to enable search,
\name{}-like systematic sweeps across repositories can also be used for analysis.  
For example, the Big Data Quality Control (BDQC) framework~\cite{deutsch2018bdqc} sweeps over 
large collections of biomedical data without regard to their meaning (domain-blind analysis) with the goal of
identifying anomalies.
BDQC employs a pipeline of extractors to derive properties of imaging, genomic, and clinical data.
While BDQC is implemented as a standalone system, the approach taken would be equally 
viable in \name{}.

\section{Approach}
\label{sec:approach}
Outline: 
- funcX: deploy endpoints on edge. 
     - Parsl: automatically provision necessary resources. 
- metadata extractors as functions w/ container runtimes. 
- Globus Auth for access control. 
- Automate (metadata extraction as part of automated workflows). 

\section{Evaluation Plan}
\label{sec:eval}

We intend to index real science use cases, particularly those in which individual data files are heterogenous 
or geographically distributed. For each, we intend to explore how to optimize the system across dimensions of 
total compute time (SLAs), monetary cost, security requirements, and resource availability.

We target the following real-world data sets: 

We have already generated an index of the Carbon Dioxide Information Analysis Center (CDIAC), a collected dataset of 
emissions data containing over 500,000 files (330+ GB) and 10,000 unique file extensions. The archive contains little 
descriptive metadata and includes a number of irrelevant files, such as debug-cycle error logs and Windows Desktop 
shortcuts.  We deployed this metadata extraction service in the cloud and moving the data to the cloud, but we expect 
to see significant time and monetary savings by moving the data to the cloud.  

The Materials Data Facility~\cite{blaiszik2016materials, blaiszik2019mdf}
is a centralized hub for publishing, sharing, and discovering materials science data. 
The MDF stores many terabytes of data from many different research groups, covering many disciplines of 
materials science, and with a diverse range of file types.
The downside of the expansive range of materials data held by the MDF 
is that it can be difficult for users to find data relevant to their science.
The MDF reduces the ``data discovery" challenge by hosting a search index that provides access to metadata from the 
files (e.g., which material was simulated in a certain calculation).
The data published by the MDF is primarily stored on storage at the National Center for Supercomputing Applications
(NCSA) and is accessible via Globus.  

Petrel? Running out of room. 

Globus GridFTP~\cite{ananthakrishnan2018globus} hosts petabytes of files over tens of thousands of individual user endpoints between research labs, 
universities, industry, and home computing devices. Many \tyler{???} users elect to have all or part of their data publicly 
shared with the rest of the Globus community. Xtract lends itself well to Globus as Globus already deploys an 
endpoint on the compute node.  We intend to deploy the funcX endpoint as part of the Globus endpoint software, and 
execute secure metadata extraction over all available petabytes. Globus is particularly useful in studying multi-site 
metadata extraction, as users generally have multiple active endpoints. \tyler{more detail}. 



\section{Conclusion}
\label{sec:conc}

Metadata extraction at the edge will make previously siloed data searchable and usable by researchers, 
industries with cumbersome data scales, and individuals simply wanting a centralized index across 
each of their files. 


\begin{acks}

This research is conducted under the guidance of Dr. Ian Foster and Dr. Kyle Chard of the 
University of Chicago and Argonne National Lab. This work is made possible in part by the contributions
of Ryan Chard, Yadu N. Babuji, and Zhuozhao Li. The existing work portions of this paper used resources of the
Argonne Leadership Computing Facility, which is a DOE Office of Science User
Facility supported under Contract DE-AC02-06CH11357. 
We gratefully acknowledge the computing resources provided and 
operated by the Joint Laboratory for System Evaluation (JLSE) at Argonne National Laboratory. 


\end{acks}
\bibliographystyle{ACM-Reference-Format}
\bibliography{wosc}


\end{document}
\endinput
