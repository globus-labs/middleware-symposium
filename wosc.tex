\documentclass[sigconf]{acmart}


\usepackage{xcolor}

\usepackage{float}
\usepackage{siunitx}


\newif\iffinal

% \finaltrue

\iffinal
  \newcommand{\tyler}[1]{}
  \newcommand{\ian}[1]{}
  \newcommand{\kyle}[1]{}
\else
  \newcommand{\tyler}[1]{{\textcolor{cyan}{ tyler: #1 }}}
  \newcommand{\ian}[1]{{\textcolor{red}{ Ian: #1 }}}
  \newcommand{\kyle}[1]{{\textcolor{purple}{ Kyle: #1 }}}
\fi


%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}


\newcommand{\name}{Xtract}
\newcommand{\funcx}{$f$\kern-0.18em\emph{unc}\kern-0.05em X}

\acmConference{Fifth International Workshop on Serverless Computing (WoSC)}{2019}{Davis, CA}

\begin{document}


\title{Harnessing Serverless to Extract File Metadata at the Edge}


\author{Tyler J. Skluzacek} 
\affiliation{University of Chicago}
\email{skluzacek@uchicago.edu}



\renewcommand{\shortauthors}{Skluzacek et al.}

\begin{abstract}
\tyler{max 100 words}

%\tyler{GRAND-er. be more dramatic. }
%\kyle{Maybe here you could say. Lots of data. Its created and stored in different places. 
%We need to rethink the siloed and antiquated file system approach and instead think
%of data in one global index of metadata (i.e., a data ocean -- just made up this term, not sure
%if I like it). To do this we need to be able extract metadata
%from where data exists. We propose a fluid, serverless middleware in which extractors
%are dynamically determined and executed wherever it makes most
%sense (e.g., at the edge, cloud, computing center, ...)}



The rapid generation of data from IoT devices, scientific instruments, and user edge devices presents
unique challenges in data management. Currently these data are distributed and generally siloed, 
but the current capabilities of file systems do not enable users to search these distributed data collections.
 In this work we propose \name{}, a serverless middleware 
that extracts metadata from files on heterogeneous edge computing resources, and conducting the extraction 
where it makes the most sense, whether on the device itself, in the cloud, or on a dedicated cluster. 
To this end, we intend to create a searchable global index across a user's or multiple users' data collections.  
	
	
%In this work
%we propose a middleware architecture called Xtract that leverages serverless 
%computing to simultaneously extract metadata across a continuum of heterogeneous edge 
%devices, from IoT to HPC to 
%laptops. Xtract will enable the curation of an atomic multi-site data lake in which the
%resources can make intelligently scale and move data in order to optimize for various 
%SLAs.


\end{abstract}

\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10002951.10003227.10010926</concept_id>
<concept_desc>Information systems~Computing platforms</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10002951.10003317.10003365.10003366</concept_id>
<concept_desc>Information systems~Search engine indexing</concept_desc>
<concept_significance>300</concept_significance>
</concept>
<concept>
<concept_id>10002951.10003317.10003318.10003319</concept_id>
<concept_desc>Information systems~Document structure</concept_desc>
<concept_significance>100</concept_significance>
</concept>
<concept>
<concept_id>10010405.10010497.10010500.10010503</concept_id>
<concept_desc>Applied computing~Document metadata</concept_desc>
<concept_significance>500</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Information systems~Computing platforms}
\ccsdesc[500]{Applied computing~Document metadata}
\ccsdesc[300]{Information systems~Search engine indexing}
\ccsdesc[100]{Information systems~Document structure}

\keywords{data lakes, serverless, metadata extraction, file systems, materials science}

\maketitle


\section{Introduction}

The rapid generation of data from IoT devices, scientific instruments, and personal computing devices presents unique 
data management challenges. Currently data are spread across multiple machines, are generally siloed, and require 
heavy manual labor by users in creating metadata that make the data on these devices searchable. Some~\cite{egan2003vizier, welter2013nhgri, irods, dataverse}  have created data catalogs from user-submitted metadata. These, however, are not scalable to future science and enterprise 
exascale use cases, as humans and ad hoc scripts provide little value in labeling billions or more heterogeneous files spanning exabytes and beyond
that would be expensive (or outright impossible) to transfer, stage, and process on small cluster file systems and personal computing devices.  
In order to build a global metadata catalog over distributed big data, we require automated methods to crawl data, extract 
metadata attributes for each file, and populate a global search index for users to find and access data. Others have developed end-to-end 
automated metadata extraction systems, but they require moving data to a central service~\cite{skluzacek2018skluma, skluzacek2016klimatic, padhy2015brown, rodrigo2018sciencesearch} or must be manually deployed at data~\cite{mattmann2011tika}. 

\tyler{un-plagiarize the serverless sentences}
In this work we propose \name{}
a serverless middleware that provides high-throughput and on-demand metadata 
extraction that enables the automated creation of rich, searchable data lakes from previously unsearchable data swamps. 
\name{} uses the \funcx{} serverless supercomputing platform~\cite{chard2019serverless}
to execute functions across diverse and distributed computing infrastructure.
Serverless computing, and in particular function as a service (FaaS),
provides an ideal model for managing the execution of
many short-running extractors on an arbitrarily large number of files. 
Serverless computing abstracts computing resources from the user, enabling
the deployment of applications without consideration for the physical and virtual infrastructure on which 
they are hosted. FaaS allows users to register programming functions with predefined input signatures.
Registered functions can subsequently be invoked many times
without the need to provision or scale any infrastructure.

%While metadata can be rapidly extracted from some data types, others, such as
%images and large hierarchical file formats can require the use of multiple extraction methods.  
%As a result, the metadata extraction process must be scalable to process large numbers
%of files, flexible to support different extraction methods, and extensible
%to be applied to various scientific data types and formats.

\tyler{unplagiarize the first 3 sentences here}
In this paper we propose the use of FaaS for mapping the metadata extraction problem to a 
collection of granular metadata extractor functions. 
We describe how such a model can support the flexibility, scalability, and extensibility required
for scientific metadata extraction. 
Rather than rely on commercial FaaS systems, we use a distributed FaaS model 
that overcomes the limitation of moving large amounts of data to the cloud. 
Instead, we are able to push
metadata extractors to the edge systems on which the scientific data reside. 
The contributions of \name{} are the following: 
\name{} is advantageous to existing metadata extraction 
systems in that it: 
\begin{itemize}
\item Is infinitely scalable across all available resources on all compute allocations. 
\item Deploys endpoints at data, allowing for the decentralization of data processing. 
\item Intelligently makes data staging decisions based on available resources across all allocations.
\item Enables users to submit custom extractor function-runtime pairs that supplement a suite of user supplied extractors.
\item Automatically populates a search index of rich, searchable metadata. 
\item Utilizes existing auth protocols for data privacy and data set sharing across multiple users. 
\end{itemize}

%Our prototype system, \name{}, provides high-throughput and on-demand metadata 
%extraction that enables the automated creation of rich, searchable data lakes from previously unsearchable data swamps. 
%\name{} uses the \funcx{} serverless supercomputing platform~\cite{chard2019serverless}
%to execute functions across diverse and distributed computing infrastructure.
%The contributions of \name{} are the following: 
%
%
%\begin{itemize}
%\item Implement and evaluate a serverless metadata extraction system
%\item Enable users to deploy endpoints and invoke metadata extraction functions on the edge
%\item Create comprehensive, searchable indexes of files across multiple devices
%\item Explore intelligent self-optimization methods subject to a number of constraints, including compute time, 
%current allocation availability, monetary cost, and security requirements (e.g., HIPAA).
%\end{itemize}

The remainder of this paper is organized as follows. 
\S\ref{sec:approach} presents our proposed architecture for \name{}. 
\S\ref{sec:eval} discusses metrics and workloads used to evaluate its performance. 
Finally, \S\ref{sec:conc} contains concluding remarks.


\section{Approach}
\label{sec:approach}
\tyler{Need to make this slightly more high-level. Probably shouldn't 
say anything about AWS in here...}
\name{} is implemented as a service via which users can submit
requests to extract metadata from a collection of files.
\name{} first crawls the specified files and determines
an initial set of extractors to apply to them. 
As outlined above, the extractors may be executed
either centrally on the \name{} server or remotely alongside
the data. As processing continues, \name{} assembles 
a metadata document for each file and dynamically selects
other extractors to apply.

%\ryan{Explain here that we have the ability to do edge but our prototype leverages petrelkube -- which is an edge device for all 
%intents and purposes as we still fire jobs into funcx}

%\name{} is implemented as a RESTful web service and is 
\name{} is deployed as a service connected to a database for storing 
intermediate metadata objects.
\name{} manages state in an AWS Relational Database Service (RDS)
instance. Each extraction request is stored in the database
and the state is updated throughout the extraction process. 
\name{} is able to send the derived metadata to an external
metadata catalog such as a Globus Search index.
The \name{} architecture is shown in Figure~\ref{fig:arch}.

\begin{figure}[t]
	\centering
	\includegraphics[scale=0.3]{figs/new-arch.png}
	\caption{Overview of the \name{} architecture. For \textit{Site A} functions are transmitted to the remote resource and performed on local computing resources, returning metadata to the \name{} service. \textit{Site B} lacks suitable local
	computing capabilities, requiring data to be staged to \name{} for analysis.}
	\label{fig:arch}
\end{figure}

\subsubsection{Metadata Extractors}
\name{} is designed to execute its extractors centrally or on edge storage systems near to where
data are stored. 
Our implementation uses the \funcx~\cite{chard2019serverless} FaaS platform 
to deploy and run extractors. 
\funcx{} is specifically designed to integrate with research computing 
cyberinfrastructure and enable a FaaS execution interface. 
\funcx{} builds upon the Parsl~\cite{babuji2019parsl} parallel programming library to 
manage the execution of functions in containers on arbitrary compute resources. 
\funcx{} enables \name{} to execute metadata extraction functions at any registered 
and accessible \funcx{} endpoint. 
We deploy the prototype with a co-located endpoint for central extraction
and use remotely deployed endpoints for edge extraction.

Each \name{} metadata extractor and its dependencies are wrapped in a Docker container
such that it can be executed in different locations. 
We have published each extractor container to \funcx{} and registered a \funcx{} function for 
each extractor. The function is responsible for invoking the extractor and returning the 
resulting metadata as a JSON dictionary. 

\funcx{} enables \name{} to reliably scale to thousands of nodes and deploy 
metadata extraction tasks on arbitrary computing resources. 
\name{} can make use of any accessible \funcx{} endpoint to process data at the edge, 
sending extractor codes and containers to the \funcx{} endpoint for execution. In addition, 
\funcx{} supports Singularity and Shifter, allowing \name{} extractors to be executed
on various high performance computing systems. 

\name{} implements a comprehensive security model using Globus Auth~\cite{tuecke2016globus}. 
All interactions with the \name{} Web service are secured with Globus Auth. 
Users can authenticate with \name{} using one of 
several hundred identity providers, including many institutions. 
\name{} uses Globus Auth OAuth~2 flows to stage data on behalf
of authenticated users. \name{} first verifies a user identity, requests an access
token to perform data transfers on their behalf, and then uses Globus to stage
data from remote storage to the \name{} extractor. 
Finally, the resulting metadata are published into the search index using a Globus Search
access token. The search index is configured with a \textit{visible\_to} field, restricting
discovery to authenticated users.


\section{Evaluation Plan}
\label{sec:eval}

We intend to index real science use cases, particularly those in which individual data files are heterogeneous 
or geographically distributed. For each, we intend to explore how to optimize the system across dimensions of 
total compute time (SLAs), monetary cost, security requirements, and resource availability.

We target the following real-world data sets: 

We have already generated an index of the Carbon Dioxide Information Analysis Center (CDIAC), a collected dataset of 
emissions data containing over 500,000 files (330+ GB) and 10,000 unique file extensions. The archive contains little 
descriptive metadata and includes a number of irrelevant files, such as debug-cycle error logs and Windows Desktop 
shortcuts.  We deployed this metadata extraction service in the cloud and moving the data to the cloud, but we expect 
to see significant time and monetary savings by moving the data to the cloud.  

The Materials Data Facility~\cite{blaiszik2016materials, blaiszik2019mdf}
is a centralized hub for publishing, sharing, and discovering materials science data. 
The MDF stores many terabytes of data from many different research groups, covering many disciplines of 
materials science, and with a diverse range of file types.
The downside of the expansive range of materials data held by the MDF 
is that it can be difficult for users to find data relevant to their science.
The MDF reduces the ``data discovery" challenge by hosting a search index that provides access to metadata from the 
files (e.g., which material was simulated in a certain calculation).

Petrel? Running out of room. 

Globus~\cite{ananthakrishnan2018globus} manages petabytes of files over tens of thousands of individual user endpoints between research labs, 
universities, industry, and home computing devices. Many \tyler{???} users elect to have all or part of their data publicly 
shared with the rest of the Globus community. \kyle{private sharing could also be indexed} Xtract lends itself well to Globus as Globus already deploys an 
endpoint on the compute node.  We intend to deploy the funcX endpoint as part of the Globus endpoint software, and 
execute secure metadata extraction over all available petabytes. Globus is particularly useful in studying multi-site 
metadata extraction, as users generally have multiple active endpoints. \tyler{more detail}. 



\section{Conclusion}
\label{sec:conc}

Metadata extraction at the edge will make previously siloed data searchable and usable by researchers, 
industries with cumbersome data scales, and individuals simply wanting a centralized index across 
each of their files. 


\begin{acks}

This research is conducted under the guidance of Dr. Ian Foster and Dr. Kyle Chard of the 
University of Chicago and Argonne National Lab. This work is made possible in part by the contributions
of Ryan Chard, Yadu N. Babuji, Zhuozhao Li, and Ryan Wong. 


\end{acks}
\bibliographystyle{ACM-Reference-Format}
\bibliography{wosc}


\end{document}
\endinput
